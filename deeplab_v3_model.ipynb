{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcefd48a-6e1a-4d0a-bf2c-2fa3f9077b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ccc155-9fdf-490f-9732-cc7a06491289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom dataset class for segmentation\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            mask_dir (str): Directory with all the masks.\n",
    "            transform (callable, optional): Optional transform to be applied on the images.\n",
    "            target_transform (callable, optional): Optional transform to be applied on the masks.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of images\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the image to be fetched.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, mask) where image is the input image and mask is the segmentation mask.\n",
    "        \"\"\"\n",
    "        # Get the image name from the list\n",
    "        img_name = self.images[idx]\n",
    "        # Construct the full path for the image\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Find the corresponding mask file\n",
    "        mask_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg']:\n",
    "            potential_mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', ext).replace('.jpeg', ext))\n",
    "            if os.path.isfile(potential_mask_path):\n",
    "                mask_path = potential_mask_path\n",
    "                break\n",
    "        \n",
    "        # Raise an error if the mask file is not found\n",
    "        if mask_path is None:\n",
    "            raise FileNotFoundError(f\"Mask file not found for image: {img_path}\")\n",
    "\n",
    "        # Open the image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        \n",
    "        # Squeeze the mask to remove the channel dimension and convert to long tensor\n",
    "        mask = mask.squeeze(0).long()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d00df83-a959-47ac-a775-bbbfdaa123fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations for images and masks\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
    "    transforms.ToTensor()           # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize masks to 128x128\n",
    "    transforms.ToTensor(),          # Convert masks to PyTorch tensors\n",
    "    transforms.Lambda(lambda x: torch.squeeze(x, 0).long())  # Squeeze channel dimension and convert to long tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c89daef-85f2-4bc2-97b8-ecebc839758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/unet_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/harsh/anaconda3/envs/unet_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects for training and validation sets\n",
    "train_dataset = SegmentationDataset('data/train/images', 'data/train/masks', transform=image_transform, target_transform=mask_transform)\n",
    "val_dataset = SegmentationDataset('data/val/images', 'data/val/masks', transform=image_transform, target_transform=mask_transform)\n",
    "\n",
    "# Create DataLoader objects for batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load a pre-trained DeepLabV3 model\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7caef311-b5cd-4bf5-9edf-d3c376594b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the classifier to match the number of classes in your dataset\n",
    "num_classes = 2  # Assuming two classes: background and rocks\n",
    "model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e26788a-40cf-490c-9ec9-9bb4badfd726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.7623\n",
      "Validation Loss: 0.5779\n",
      "Epoch [2/25], Loss: 0.6942\n",
      "Validation Loss: 1.1191\n",
      "Epoch [3/25], Loss: 0.6178\n",
      "Validation Loss: 0.9698\n",
      "Epoch [4/25], Loss: 0.5349\n",
      "Validation Loss: 0.0216\n",
      "Epoch [5/25], Loss: 0.4636\n",
      "Validation Loss: 0.0236\n",
      "Epoch [6/25], Loss: 0.3688\n",
      "Validation Loss: 0.1501\n",
      "Epoch [7/25], Loss: 0.3096\n",
      "Validation Loss: 0.2765\n",
      "Epoch [8/25], Loss: 0.2613\n",
      "Validation Loss: 0.3547\n",
      "Epoch [9/25], Loss: 0.2209\n",
      "Validation Loss: 0.3120\n",
      "Epoch [10/25], Loss: 0.1897\n",
      "Validation Loss: 0.2500\n",
      "Epoch [11/25], Loss: 0.1637\n",
      "Validation Loss: 0.1928\n",
      "Epoch [12/25], Loss: 0.1418\n",
      "Validation Loss: 0.1513\n",
      "Epoch [13/25], Loss: 0.1241\n",
      "Validation Loss: 0.1349\n",
      "Epoch [14/25], Loss: 0.1097\n",
      "Validation Loss: 0.1334\n",
      "Epoch [15/25], Loss: 0.0976\n",
      "Validation Loss: 0.1321\n",
      "Epoch [16/25], Loss: 0.0874\n",
      "Validation Loss: 0.1293\n",
      "Epoch [17/25], Loss: 0.0778\n",
      "Validation Loss: 0.1258\n",
      "Epoch [18/25], Loss: 0.0701\n",
      "Validation Loss: 0.1202\n",
      "Epoch [19/25], Loss: 0.0636\n",
      "Validation Loss: 0.1119\n",
      "Epoch [20/25], Loss: 0.0583\n",
      "Validation Loss: 0.1023\n",
      "Epoch [21/25], Loss: 0.0535\n",
      "Validation Loss: 0.0912\n",
      "Epoch [22/25], Loss: 0.0487\n",
      "Validation Loss: 0.0819\n",
      "Epoch [23/25], Loss: 0.0448\n",
      "Validation Loss: 0.0742\n",
      "Epoch [24/25], Loss: 0.0417\n",
      "Validation Loss: 0.0678\n",
      "Epoch [25/25], Loss: 0.0386\n",
      "Validation Loss: 0.0622\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        outputs = model(images)['out']  # Forward pass\n",
    "        loss = criterion(outputs, masks)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation step (optional but recommended)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834f6315-6112-4f11-9c9d-49c4c1531183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'deeplabv3_rock_detection.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "torch.save(model.state_dict(), 'deeplabv3_rock_detection.pth')\n",
    "print(\"Model saved to 'deeplabv3_rock_detection.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c251463-7c4b-4d5f-9fde-c66284525626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for inference\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model for inference (example usage)\n",
    "model.load_state_dict(torch.load('deeplabv3_rock_detection.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded for inference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
